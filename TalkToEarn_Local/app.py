# app.py - åŸºäºAILibrariesçš„å¤šç”¨æˆ·AIçŸ¥è¯†åº“åˆ†äº«å¹³å°
import os
import json
import numpy as np
from flask import Flask, request, jsonify, Response, render_template, session, redirect, url_for
import chardet
import time
from langchain_core.documents import Document
import uuid
from werkzeug.utils import secure_filename
import math
import hashlib
from datetime import datetime

# ==================== å¯¼å…¥å¿…è¦çš„åº“ ====================
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, OllamaLLM

app = Flask(__name__)
app.secret_key = 'your-secret-key-here'

# ==================== æ–‡ä»¶è·¯å¾„é…ç½® ====================
UPLOAD_FOLDER = 'USER_DATA'
SHARED_FOLDER = 'SHARED_CONTENT'
USER_DB_FILE = 'users.json'
FILES_DB_FILE = 'files.json'
TRANSACTIONS_DB_FILE = 'transactions.json'

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(SHARED_FOLDER, exist_ok=True)

# ==================== Ollama é…ç½® ====================
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434')

embeddings = OllamaEmbeddings(
    model='mxbai-embed-large:latest',
    base_url=OLLAMA_HOST
)

llm = OllamaLLM(
    model='deepseek-r1:1.5b',
    temperature=0.3,
    base_url=OLLAMA_HOST
)

vector_store = None

# ==================== ç”¨æˆ·ç®¡ç†ç³»ç»Ÿ ====================

def load_users():
    if os.path.exists(USER_DB_FILE):
        with open(USER_DB_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_users(users):
    with open(USER_DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(users, f, ensure_ascii=False, indent=2)

def load_files():
    if os.path.exists(FILES_DB_FILE):
        with open(FILES_DB_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_files(files):
    with open(FILES_DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(files, f, ensure_ascii=False, indent=2)

def load_transactions():
    if os.path.exists(TRANSACTIONS_DB_FILE):
        with open(TRANSACTIONS_DB_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_transactions(transactions):
    with open(TRANSACTIONS_DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(transactions, f, ensure_ascii=False, indent=2)

def hash_password(password):
    return hashlib.sha256(password.encode()).hexdigest()

def register_user(user_id, password):
    users = load_users()
    
    if user_id in users:
        return False, "ç”¨æˆ·IDå·²å­˜åœ¨"
    
    # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿æ–°ç”¨æˆ·çš„æ‰€æœ‰ç»Ÿè®¡å­—æ®µéƒ½æ­£ç¡®åˆå§‹åŒ–
    users[user_id] = {
        'password_hash': hash_password(password),
        'coin_balance': 1.0,
        'total_earned': 0.0,  # ğŸ¯ ç¡®ä¿åˆå§‹åŒ–ä¸º0
        'total_spent': 0.0,   # ğŸ¯ ç¡®ä¿åˆå§‹åŒ–ä¸º0
        'registration_time': datetime.now().isoformat(),
        'uploaded_files': [],
        'referenced_files': []  # ğŸ¯ ç¡®ä¿è¿™ä¸ªå­—æ®µå­˜åœ¨
    }
    
    save_users(users)
    return True, "æ³¨å†ŒæˆåŠŸ"

def authenticate_user(user_id, password):
    users = load_users()
    
    if user_id not in users:
        return False, "ç”¨æˆ·ä¸å­˜åœ¨"
    
    user_data = users[user_id]
    if not isinstance(user_data, dict) or 'password_hash' not in user_data:
        return False, "ç”¨æˆ·æ•°æ®ä¸å®Œæ•´ï¼Œè¯·é‡æ–°æ³¨å†Œ"
    
    if user_data['password_hash'] != hash_password(password):
        return False, "å¯†ç é”™è¯¯"
    
    return True, "ç™»å½•æˆåŠŸ"

def get_user_stats(user_id):
    users = load_users()
    if user_id not in users:
        return None
    
    user = users[user_id]
    transactions = load_transactions()
    today = datetime.now().date()
    
    today_earned = 0.0
    today_references = 0
    
    for tx in transactions:
        tx_time = datetime.fromisoformat(tx['timestamp']).date()
        if tx_time == today:
            if tx['type'] == 'reward' and tx['to_user'] == user_id:
                today_earned += tx['amount']
            elif tx['type'] == 'reference' and tx['file_owner'] == user_id:
                today_references += 1
    
    return {
        'coin_balance': user['coin_balance'],
        'total_earned': user['total_earned'],
        'total_spent': user['total_spent'],
        'today_earned': today_earned,
        'today_references': today_references,
        'uploaded_files_count': len(user['uploaded_files'])
    }


def calculate_user_earnings(user_id):
    """é‡æ–°è®¡ç®—ç”¨æˆ·çš„æ€»æ”¶ç›Š - ä¿®å¤ç»Ÿè®¡é—®é¢˜"""
    users = load_users()
    transactions = load_transactions()
    
    if user_id not in users:
        return 0.0, 0.0, 0
    
    total_earned = 0.0
    total_spent = 0.0
    reference_count = 0
    
    # é‡æ–°è®¡ç®—æ‰€æœ‰äº¤æ˜“
    for tx in transactions:
        # è®¡ç®—æ”¶ç›Šï¼ˆå¥–åŠ±å’Œå¼•ç”¨ï¼‰
        if tx['to_user'] == user_id and tx['type'] in ['reward', 'reference']:
            total_earned += tx['amount']
            if tx['type'] == 'reference':
                reference_count += 1
        # è®¡ç®—æ”¯å‡º
        elif tx['from_user'] == user_id and tx['type'] == 'spend':
            total_spent += tx['amount']
    
    # æ›´æ–°ç”¨æˆ·æ•°æ®
    users[user_id]['total_earned'] = total_earned
    users[user_id]['total_spent'] = total_spent
    
    # ç¡®ä¿ä½™é¢æ­£ç¡®
    initial_balance = 1.0  # æ³¨å†Œæ—¶èµ é€çš„1coin
    calculated_balance = initial_balance + total_earned - total_spent
    users[user_id]['coin_balance'] = max(0, calculated_balance)  # ä½™é¢ä¸èƒ½ä¸ºè´Ÿ
    
    save_users(users)
    
    print(f"ğŸ’° ç”¨æˆ· {user_id} æ”¶ç›Šç»Ÿè®¡: æ€»æ”¶ç›Š={total_earned:.6f}, æ€»æ”¯å‡º={total_spent:.6f}, å¼•ç”¨æ¬¡æ•°={reference_count}")
    
    return total_earned, total_spent, reference_count


def record_transaction(tx_type, from_user, to_user, amount, file_owner=None, file_id=None, question=None):
    """ä¿®å¤äº¤æ˜“è®°å½•å‡½æ•° - ç¡®ä¿ä½™é¢æ­£ç¡®æ›´æ–°"""
    transactions = load_transactions()
    
    transaction = {
        'id': str(uuid.uuid4()),
        'type': tx_type,
        'from_user': from_user,
        'to_user': to_user,
        'amount': amount,
        'file_owner': file_owner,
        'file_id': file_id,
        'question': question,
        'timestamp': datetime.now().isoformat()
    }
    
    transactions.append(transaction)
    save_transactions(transactions)
    
    print(f"ğŸ’¾ è®°å½•äº¤æ˜“: {tx_type}, ä» {from_user} åˆ° {to_user}, é‡‘é¢ {amount:.8f}")
    
    # ğŸ¯ ä¿®å¤ï¼šé‡æ–°åŠ è½½æœ€æ–°çš„ç”¨æˆ·æ•°æ®
    users = load_users()
    
    if tx_type == 'spend' and from_user in users:
        # ç¡®ä¿ä½™é¢ä¸ä¼šå˜æˆè´Ÿæ•°
        new_balance = max(0, users[from_user]['coin_balance'] - amount)
        users[from_user]['coin_balance'] = new_balance
        users[from_user]['total_spent'] += amount
        print(f"ğŸ’¸ ç”¨æˆ· {from_user} æ”¯å‡º {amount:.8f}, æ–°ä½™é¢: {users[from_user]['coin_balance']:.6f}")
    
    if tx_type == 'reward' and to_user in users:
        users[to_user]['coin_balance'] += amount
        users[to_user]['total_earned'] += amount
        print(f"ğŸ ç”¨æˆ· {to_user} è·å¾—å¥–åŠ± {amount:.8f}, æ–°ä½™é¢: {users[to_user]['coin_balance']:.6f}")
    
    # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ä¿å­˜
    save_users(users)
    
    # ğŸ¯ ä¿®å¤ï¼šå†æ¬¡éªŒè¯æ•°æ®æ˜¯å¦ä¿å­˜æˆåŠŸ
    users_after_save = load_users()
    if to_user in users_after_save and tx_type == 'reward':
        print(f"âœ… æœ€ç»ˆéªŒè¯: ç”¨æˆ· {to_user} ä½™é¢å·²æ›´æ–°ä¸º {users_after_save[to_user]['coin_balance']:.6f}")
    if from_user in users_after_save and tx_type == 'spend':
        print(f"âœ… æœ€ç»ˆéªŒè¯: ç”¨æˆ· {from_user} ä½™é¢å·²æ›´æ–°ä¸º {users_after_save[from_user]['coin_balance']:.6f}")

@app.route('/profile')
def user_profile():
    if 'user_id' not in session:
        return redirect('/login')
    
    user_id = session['user_id']
    
    # ğŸ¯ é‡æ–°è®¡ç®—ç”¨æˆ·æ”¶ç›Šç¡®ä¿æ•°æ®å‡†ç¡®
    total_earned, total_spent, _ = calculate_user_earnings(user_id)
    
    # é‡æ–°åŠ è½½æœ€æ–°æ•°æ®
    users = load_users()
    
    if user_id not in users:
        return redirect('/logout')
    
    user = users[user_id]
    
    # ç¡®ä¿ç”¨æˆ·æ•°æ®ç»“æ„å®Œæ•´
    if 'total_earned' not in user:
        user['total_earned'] = 0.0
    if 'total_spent' not in user:
        user['total_spent'] = 0.0
    if 'referenced_files' not in user:
        user['referenced_files'] = []
    
    transactions = load_transactions()
    
    # è·å–ç”¨æˆ·çš„äº¤æ˜“è®°å½•
    user_transactions = []
    for tx in transactions:
        if tx['from_user'] == user_id or tx['to_user'] == user_id:
            user_transactions.append(tx)
    
    # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œå–æœ€è¿‘20æ¡
    user_transactions.sort(key=lambda x: x['timestamp'], reverse=True)
    recent_transactions = user_transactions[:20]
    
    # è·å–ç”¨æˆ·æ–‡ä»¶å¼•ç”¨ç»Ÿè®¡
    user_files = search_files(user_id=user_id)
    reference_stats = []
    
    for file_info in user_files:
        file_references = [tx for tx in transactions 
                          if tx.get('file_id') == file_info['file_id'] and tx['type'] == 'reference']
        reference_stats.append({
            'file_id': file_info['file_id'],
            'filename': file_info['filename'],
            'reference_count': len(file_references),
            'total_reward': file_info.get('total_reward', 0)
        })
    
    # è®¡ç®—ä»Šæ—¥æ”¶ç›Š
    today = datetime.now().date()
    today_earned = 0.0
    today_references = 0
    
    for tx in transactions:
        if tx['to_user'] == user_id and tx['type'] == 'reward':
            tx_time = datetime.fromisoformat(tx['timestamp']).date()
            if tx_time == today:
                today_earned += tx['amount']
        elif tx.get('file_owner') == user_id and tx['type'] == 'reference':
            tx_time = datetime.fromisoformat(tx['timestamp']).date()
            if tx_time == today:
                today_references += 1
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ“Š Profileé¡µé¢ - ç”¨æˆ·: {user_id}")
    print(f"ğŸ’° ä½™é¢: {user['coin_balance']:.6f}")
    print(f"ğŸ“ˆ æ€»æ”¶ç›Š: {user['total_earned']:.6f}")
    print(f"ğŸ“‰ æ€»æ”¯å‡º: {user['total_spent']:.6f}")
    print(f"ğŸ“ æ–‡ä»¶æ•°: {len(user_files)}")
    print(f"ğŸ“‹ äº¤æ˜“è®°å½•æ•°: {len(recent_transactions)}")
    print(f"ğŸ¯ ä»Šæ—¥æ”¶ç›Š: {today_earned:.6f}, ä»Šæ—¥å¼•ç”¨: {today_references}")
    
    return render_template('profile.html',
                         user_id=user_id,
                         user=user,
                         transactions=recent_transactions,
                         reference_stats=reference_stats,
                         today_earned=today_earned,
                         today_references=today_references)


# ==================== æ–‡ä»¶ç®¡ç†ç³»ç»Ÿ ====================
def save_shared_file(user_id, filename, content, authorize_rag=True):
    files = load_files()
    
    # ç”Ÿæˆæ–‡ä»¶ID - ç¡®ä¿æ ¼å¼æ­£ç¡®
    file_id = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{user_id}"
    
    # åˆ›å»ºæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨æ–‡ä»¶IDä½œä¸ºæ–‡ä»¶å
    filepath = os.path.join(SHARED_FOLDER, f"{file_id}.txt")
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    files[file_id] = {
        'filename': filename,
        'user_id': user_id,
        'content': content,
        'content_preview': content[:200] + "..." if len(content) > 200 else content,
        'upload_time': datetime.now().isoformat(),
        'authorize_rag': authorize_rag,
        'reference_count': 0,
        'total_reward': 0.0,
        'file_path': filepath
    }
    
    save_files(files)
    
    users = load_users()
    if user_id in users:
        users[user_id]['uploaded_files'].append(file_id)
        save_users(users)
    
    if authorize_rag:
        try:
            print(f"å¼€å§‹å°†æ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“: {file_id}, æ–‡ä»¶å: {filename}")
            add_file_to_vector_store(filepath, file_id, user_id, filename)
            print(f"æˆåŠŸå°†æ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“: {file_id}")
        except Exception as e:
            print(f"æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    return file_id

def add_file_to_vector_store(filepath, file_id, user_id, filename):
    global vector_store
    
    try:
        init_vector_store(filepath)
        print(f"æˆåŠŸæ·»åŠ æ–‡ä»¶åˆ°çŸ¥è¯†åº“: {filename}")
    except Exception as e:
        print(f"æ·»åŠ æ–‡ä»¶åˆ°å‘é‡åº“å¤±è´¥: {e}")
        raise

# åœ¨ app.py ä¸­æ‰¾åˆ° search_files å‡½æ•°ï¼Œå¹¶è¿›è¡Œç±»ä¼¼å¦‚ä¸‹ä¿®æ”¹
def search_files(file_id=None, user_id=None, keyword=None):
    files = load_files()
    results = []
    
    for fid, file_info in files.items():
        match = True
        
        if file_id and fid != file_id:
            match = False
        if user_id and file_info['user_id'] != user_id:
            match = False
        if keyword:
            # æ‰©å±•æœç´¢èŒƒå›´ï¼šåŒæ—¶åŒ¹é…æ–‡ä»¶IDã€æ–‡ä»¶åå’Œæ–‡ä»¶å†…å®¹
            keyword_lower = keyword.lower()
            file_id_match = (fid.lower().find(keyword_lower) != -1)
            filename_match = (file_info['filename'].lower().find(keyword_lower) != -1)
            content_match = (file_info['content'].lower().find(keyword_lower) != -1)
            
            if not (file_id_match or filename_match or content_match):
                match = False
                
        if match:
            results.append({
                'file_id': fid,
                **file_info
            })
    
    return sorted(results, key=lambda x: x['upload_time'], reverse=True)


# ==================== æ™ºèƒ½å¥–åŠ±åˆ†é…ç³»ç»Ÿ ====================

def calculate_reward_distribution(relevant_docs, total_cost):
    """ä¿®å¤å¥–åŠ±è®¡ç®—å‡½æ•°"""
    if not relevant_docs:
        print("âš ï¸ æ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•åˆ†é…å¥–åŠ±")
        return {}
    
    similarities = []
    file_similarities = {}
    
    print(f"ğŸ“Š å¼€å§‹è®¡ç®—å¥–åŠ±åˆ†å¸ƒ: æ€»æˆæœ¬ {total_cost}, æ–‡æ¡£æ•° {len(relevant_docs)}")
    
    for doc in relevant_docs:
        file_id = doc.metadata.get('file_id')
        similarity = doc.metadata.get('semantic_similarity', 0.3)
        
        print(f"ğŸ“„ æ–‡æ¡£ {file_id}: ç›¸ä¼¼åº¦ {similarity:.3f}")
        
        if file_id:
            if file_id not in file_similarities:
                file_similarities[file_id] = []
            file_similarities[file_id].append(similarity)
            similarities.append(similarity)
    
    if not similarities:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç›¸ä¼¼åº¦æ•°æ®")
        return {}
    
    # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å¹³å‡ç›¸ä¼¼åº¦
    file_avg_similarities = {}
    for file_id, sim_list in file_similarities.items():
        file_avg_similarities[file_id] = sum(sim_list) / len(sim_list)
        print(f"ğŸ“ˆ æ–‡ä»¶ {file_id}: å¹³å‡ç›¸ä¼¼åº¦ {file_avg_similarities[file_id]:.3f}")
    
    total_similarity = sum(file_avg_similarities.values())
    print(f"ğŸ“Š æ€»ç›¸ä¼¼åº¦: {total_similarity:.3f}")
    
    if total_similarity == 0:
        print("âš ï¸ æ€»ç›¸ä¼¼åº¦ä¸º0ï¼Œæ— æ³•åˆ†é…å¥–åŠ±")
        return {}
    
    reward_distribution = {}
    for file_id, avg_similarity in file_avg_similarities.items():
        weight = avg_similarity / total_similarity
        reward = weight * total_cost
        
        print(f"ğŸ’° æ–‡ä»¶ {file_id}: æƒé‡ {weight:.3f}, å¥–åŠ± {reward:.8f} coin")
        
        reward_distribution[file_id] = {
            'reward': reward,
            'weight': weight,
            'similarity': avg_similarity
        }
    
    total_distributed = sum(info['reward'] for info in reward_distribution.values())
    print(f"ğŸ¯ æ€»åˆ†é…é‡‘é¢: {total_distributed:.8f} coin")
    
    return reward_distribution

def distribute_rewards(user_id, question, relevant_docs, total_cost):
    """ä¿®å¤å¥–åŠ±åˆ†é…å‡½æ•° - ç¡®ä¿å¥–åŠ±æ­£ç¡®åˆ†é…å’Œè®°å½•"""
    reward_distribution = calculate_reward_distribution(relevant_docs, total_cost)
    
    files = load_files()
    users = load_users()
    transactions = load_transactions()
    
    distribution_info = {}
    total_distributed = 0.0
    
    print(f"ğŸ” å¼€å§‹å¥–åŠ±åˆ†é…: æ€»æˆæœ¬ {total_cost}, ç›¸å…³æ–‡æ¡£ {len(relevant_docs)} ä¸ª")
    
    for file_id, reward_info in reward_distribution.items():
        if file_id and file_id in files:
            file_owner = files[file_id]['user_id']
            reward_amount = reward_info['reward']
            
            if reward_amount > 0 and file_owner in users:
                try:
                    # ğŸ¯ ä¿®å¤ï¼šç›´æ¥æ›´æ–°ç”¨æˆ·ä½™é¢
                    users[file_owner]['coin_balance'] += reward_amount
                    if 'total_earned' not in users[file_owner]:
                        users[file_owner]['total_earned'] = 0.0
                    users[file_owner]['total_earned'] += reward_amount
                    
                    # è®°å½•å¥–åŠ±äº¤æ˜“
                    reward_tx = {
                        'id': str(uuid.uuid4()),
                        'type': 'reward',
                        'from_user': None,  # ç³»ç»Ÿå‘æ”¾
                        'to_user': file_owner,
                        'amount': reward_amount,
                        'file_owner': file_owner,
                        'file_id': file_id,
                        'question': question,
                        'timestamp': datetime.now().isoformat()
                    }
                    transactions.append(reward_tx)
                    
                    # è®°å½•å¼•ç”¨äº¤æ˜“
                    reference_tx = {
                        'id': str(uuid.uuid4()),
                        'type': 'reference',
                        'from_user': user_id,
                        'to_user': file_owner,
                        'amount': 0.0,  # å¼•ç”¨è®°å½•ï¼Œé‡‘é¢ä¸º0
                        'file_owner': file_owner,
                        'file_id': file_id,
                        'question': question,
                        'timestamp': datetime.now().isoformat()
                    }
                    transactions.append(reference_tx)
                    
                    # æ›´æ–°æ–‡ä»¶ç»Ÿè®¡
                    files[file_id]['reference_count'] += 1
                    files[file_id]['total_reward'] += reward_amount
                    
                    total_distributed += reward_amount
                    
                    print(f"âœ… æˆåŠŸåˆ†é…å¥–åŠ±: {file_owner} è·å¾— {reward_amount:.8f} coin")
                    
                except Exception as e:
                    print(f"âŒ å¥–åŠ±åˆ†é…å¤±è´¥ {file_id}: {e}")
    
    # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ä¿å­˜
    save_files(files)
    save_users(users)
    save_transactions(transactions)
    
    print(f"ğŸ¯ å¥–åŠ±åˆ†é…å®Œæˆ: æ€»åˆ†é…é‡‘é¢ {total_distributed:.8f} coin")
    return distribution_info

def extract_file_id_from_source(source):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–file_id"""
    if not source:
        return None
    
    # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆä¸å¸¦æ‰©å±•åï¼‰
    filename = os.path.basename(source)
    if '.' in filename:
        file_id = filename.split('.')[0]  # å»æ‰æ‰©å±•å
    else:
        file_id = filename
    
    print(f"ğŸ” ä»sourceæå–file_id: {source} -> {file_id}")
    return file_id

def calculate_reward_distribution(relevant_docs, total_cost):
    """ä¿®å¤å¥–åŠ±è®¡ç®—å‡½æ•° - å¤„ç†file_idä¸ºNoneçš„æƒ…å†µ"""
    if not relevant_docs:
        print("âš ï¸ æ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•åˆ†é…å¥–åŠ±")
        return {}
    
    similarities = []
    file_similarities = {}
    
    print(f"ğŸ“Š å¼€å§‹è®¡ç®—å¥–åŠ±åˆ†å¸ƒ: æ€»æˆæœ¬ {total_cost}, æ–‡æ¡£æ•° {len(relevant_docs)}")
    
    for doc in relevant_docs:
        file_id = doc.metadata.get('file_id')
        similarity = doc.metadata.get('semantic_similarity', 0.3)
        
        # å¦‚æœfile_idä¸ºNoneï¼Œå°è¯•ä»sourceä¸­æå–
        if file_id is None:
            source = doc.metadata.get('source', '')
            file_id = extract_file_id_from_source(source)
            print(f"ğŸ”„ è®¡ç®—å¥–åŠ±æ—¶æå–file_id: {source} -> {file_id}")
        
        print(f"ğŸ“„ æ–‡æ¡£ {file_id}: ç›¸ä¼¼åº¦ {similarity:.3f}")
        
        if file_id:
            if file_id not in file_similarities:
                file_similarities[file_id] = []
            file_similarities[file_id].append(similarity)
            similarities.append(similarity)
    
    if not similarities:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç›¸ä¼¼åº¦æ•°æ®")
        return {}
    
    # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å¹³å‡ç›¸ä¼¼åº¦
    file_avg_similarities = {}
    for file_id, sim_list in file_similarities.items():
        file_avg_similarities[file_id] = sum(sim_list) / len(sim_list)
        print(f"ğŸ“ˆ æ–‡ä»¶ {file_id}: å¹³å‡ç›¸ä¼¼åº¦ {file_avg_similarities[file_id]:.3f}")
    
    total_similarity = sum(file_avg_similarities.values())
    print(f"ğŸ“Š æ€»ç›¸ä¼¼åº¦: {total_similarity:.3f}")
    
    if total_similarity == 0:
        print("âš ï¸ æ€»ç›¸ä¼¼åº¦ä¸º0ï¼Œæ— æ³•åˆ†é…å¥–åŠ±")
        return {}
    
    reward_distribution = {}
    for file_id, avg_similarity in file_avg_similarities.items():
        weight = avg_similarity / total_similarity
        reward = weight * total_cost
        
        print(f"ğŸ’° æ–‡ä»¶ {file_id}: æƒé‡ {weight:.3f}, å¥–åŠ± {reward:.8f} coin")
        
        reward_distribution[file_id] = {
            'reward': reward,
            'weight': weight,
            'similarity': avg_similarity
        }
    
    total_distributed = sum(info['reward'] for info in reward_distribution.values())
    print(f"ğŸ¯ æ€»åˆ†é…é‡‘é¢: {total_distributed:.8f} coin")
    
    return reward_distribution



# ==================== ä»AILibrarieså¤åˆ¶çš„æ ¸å¿ƒAIåŠŸèƒ½ ====================

def enhanced_cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1).flatten()
    vec2 = np.array(vec2).flatten()
    
    if np.all(vec1 == 0) or np.all(vec2 == 0):
        return 0.0
    
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    
    similarity = dot_product / (norm_vec1 * norm_vec2)
    similarity = max(-1.0, min(1.0, similarity))
    
    return float(similarity)

def llm_based_relevance_check(question, document_content, llm_model):
    try:
        truncated_content = document_content[:800] + "..." if len(document_content) > 800 else document_content
        
        prompt = f"""è¯·ä¸¥æ ¼åˆ¤æ–­ä»¥ä¸‹æ–‡æ¡£å†…å®¹æ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ã€‚è¯·åªå›ç­”"ç›¸å…³"æˆ–"ä¸ç›¸å…³"ï¼Œä¸è¦è§£é‡Šã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

æ–‡æ¡£å†…å®¹ï¼š{truncated_content}

è¯·åˆ¤æ–­æ–‡æ¡£å†…å®¹æ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ï¼Œåªå›ç­”"ç›¸å…³"æˆ–"ä¸ç›¸å…³"ï¼š"""
        
        response = llm_model.invoke(prompt).strip().lower()
        print(f"LLMç›¸å…³æ€§åˆ¤æ–­ç»“æœ: '{response}'")
        
        return "ç›¸å…³" in response and "ä¸ç›¸å…³" not in response
        
    except Exception as e:
        print(f"LLMç›¸å…³æ€§åˆ¤æ–­é”™è¯¯: {e}")
        return False

def hybrid_relevance_check(question, doc, embeddings_model, llm_model):
    semantic_similarity = calculate_semantic_similarity(question, doc.page_content, embeddings_model)
    
    if semantic_similarity > 0.7:
        return True, semantic_similarity
    elif semantic_similarity > 0.4:
        is_llm_relevant = llm_based_relevance_check(question, doc.page_content, llm_model)
        return is_llm_relevant, semantic_similarity
    else:
        return False, semantic_similarity

def calculate_jaccard_similarity(text1, text2):
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    if not words1 and not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return intersection / union if union > 0 else 0.0

def calculate_semantic_similarity(question, document_content, embeddings_model):
    try:
        question_embedding = embeddings_model.embed_query(question)
        doc_embedding = embeddings_model.embed_query(document_content)
        
        base_similarity = enhanced_cosine_similarity(question_embedding, doc_embedding)
        
        is_conceptual_question = any(keyword in question for keyword in 
                                    ["ä»€ä¹ˆæ˜¯", "ä»€ä¹ˆå«", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š"])
        
        doc_length = len(document_content.split())
        if is_conceptual_question:
            length_factor = min(1.0, doc_length / 25)
        else:
            length_factor = min(1.0, doc_length / 40)
        
        jaccard_similarity = calculate_jaccard_similarity(question, document_content)
        
        concept_keywords = {
            "çˆ±": ["çˆ±", "çˆ±æƒ…", "çˆ±å¿ƒ", "å…³çˆ±", "çƒ­çˆ±", "æƒ…æ„Ÿ", "æ„Ÿæƒ…", "å…³ç³»", "äº²å¯†", "å®šä¹‰", "æ¦‚å¿µ"],
            "ä»€ä¹ˆæ˜¯": ["å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š", "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆå«", "æ„å‘³ç€", "æŒ‡çš„æ˜¯"]
        }
        
        keyword_boost = 0.0
        for concept, keywords in concept_keywords.items():
            if concept in question:
                keyword_matches = sum(1 for keyword in keywords if keyword in document_content)
                if keyword_matches > 0:
                    if is_conceptual_question:
                        keyword_boost = min(0.25, keyword_matches * 0.08)
                    else:
                        keyword_boost = min(0.15, keyword_matches * 0.05)
                    print(f"å…³é”®è¯åŒ¹é…å¢å¼º: åŒ¹é…åˆ° {keyword_matches} ä¸ªç›¸å…³å…³é”®è¯ï¼Œæå‡ {keyword_boost:.3f}")
                    break
        
        question_len = len(question)
        doc_len = len(document_content)
        if question_len > 0 and doc_len > 0:
            length_similarity = 1 - abs(question_len - doc_len) / (question_len + doc_len)
        else:
            length_similarity = 0
        
        if is_conceptual_question:
            semantic_similarity = (
                0.75 * base_similarity +
                0.05 * jaccard_similarity +
                0.1 * length_factor +
                0.1 * length_similarity +
                keyword_boost
            )
            semantic_similarity = 1 / (1 + math.exp(-6 * (semantic_similarity - 0.4)))
        else:
            semantic_similarity = (
                0.8 * base_similarity +
                0.05 * jaccard_similarity +
                0.1 * length_factor +
                0.05 * length_similarity +
                keyword_boost
            )
            semantic_similarity = 1 / (1 + math.exp(-10 * (semantic_similarity - 0.55)))
        
        print(f"ç›¸ä¼¼åº¦åˆ†è§£ - è¯­ä¹‰:{base_similarity:.3f}, Jaccard:{jaccard_similarity:.3f}, é•¿åº¦å› å­:{length_factor:.3f}, å…³é”®è¯å¢å¼º:{keyword_boost:.3f}, ç»¼åˆ:{semantic_similarity:.3f}")
        
        return semantic_similarity
        
    except Exception as e:
        print(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
        return 0.4

def adaptive_filter_relevant_docs(question, docs, embeddings_model, llm_model):
    relevant_docs = []
    
    print(f"å¼€å§‹è‡ªé€‚åº”è¿‡æ»¤ {len(docs)} ä¸ªæ–‡æ¡£")
    
    is_conceptual_question = any(keyword in question for keyword in 
                                ["ä»€ä¹ˆæ˜¯", "ä»€ä¹ˆå«", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š", "ä¸ºä»€ä¹ˆ"])
    
    if is_conceptual_question:
        print("æ£€æµ‹åˆ°æ¦‚å¿µæ€§é—®é¢˜ï¼Œé‡‡ç”¨LLMä¸»å¯¼çš„è¿‡æ»¤ç­–ç•¥")
    
    for i, doc in enumerate(docs):
        try:
            is_relevant, similarity = hybrid_relevance_check(question, doc, embeddings_model, llm_model)
            
            doc_preview = doc.page_content[:50] + "..." if len(doc.page_content) > 50 else doc.page_content
            print(f"æ–‡æ¡£ {i+1} æ··åˆç›¸ä¼¼åº¦: {similarity:.3f}, ç›¸å…³: {is_relevant} - å†…å®¹: {doc_preview}")
            
            if is_relevant:
                doc.metadata['semantic_similarity'] = float(similarity)
                relevant_docs.append((similarity, doc))
                
        except Exception as e:
            print(f"æ–‡æ¡£ {i+1} ç›¸å…³æ€§åˆ¤æ–­é”™è¯¯: {e}")
            doc.metadata['semantic_similarity'] = 0.4
            relevant_docs.append((0.4, doc))
    
    if not relevant_docs:
        return []
    
    relevant_docs.sort(key=lambda x: x[0], reverse=True)
    
    llm_relevant_docs = [doc for similarity, doc in relevant_docs]
    
    if is_conceptual_question:
        max_docs = min(6, len(llm_relevant_docs))
        filtered_docs = llm_relevant_docs[:max_docs]
        print(f"æ¦‚å¿µæ€§é—®é¢˜ - ä¿ç•™æ‰€æœ‰LLMåˆ¤æ–­ç›¸å…³çš„æ–‡æ¡£: {len(filtered_docs)} ä¸ª")
    else:
        similarities = [similarity for similarity, doc in relevant_docs]
        if len(similarities) > 0:
            avg_similarity = sum(similarities) / len(similarities)
            dynamic_threshold = max(0.40, avg_similarity + 0.2 * math.sqrt(sum((x - avg_similarity) ** 2 for x in similarities) / len(similarities)))
            filtered_docs = [doc for similarity, doc in relevant_docs if similarity >= dynamic_threshold]
            filtered_docs = filtered_docs[:4]
            print(f"æ™®é€šé—®é¢˜ - åŠ¨æ€é˜ˆå€¼: {dynamic_threshold:.3f}, ä¿ç•™: {len(filtered_docs)} ä¸ªæ–‡æ¡£")
        else:
            filtered_docs = llm_relevant_docs[:3]
    
    print(f"è¿‡æ»¤åä¿ç•™ {len(filtered_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
    return filtered_docs

def intelligent_rag_decision(question, relevant_docs):
    if not relevant_docs:
        return False, "æ²¡æœ‰ç›¸å…³æ–‡æ¡£", 0.0
    
    similarities = [doc.metadata.get('semantic_similarity', 0) for doc in relevant_docs]
    max_similarity = max(similarities) if similarities else 0
    avg_similarity = sum(similarities) / len(similarities) if similarities else 0
    
    print(f"RAGå†³ç­– - æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}, å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
    
    is_conceptual_question = any(keyword in question for keyword in 
                                ["ä»€ä¹ˆæ˜¯", "ä»€ä¹ˆå«", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š", "ä¸ºä»€ä¹ˆ"])
    
    if is_conceptual_question:
        if len(relevant_docs) == 0:
            return False, "æ²¡æœ‰ç›¸å…³æ–‡æ¡£", 0.0
        else:
            doc_count_factor = min(1.0, len(relevant_docs) / 3.0)
            similarity_factor = min(1.0, max_similarity / 0.7)
            
            confidence = 0.5 + 0.3 * doc_count_factor + 0.2 * similarity_factor
            confidence = min(0.9, confidence)
            
            return True, f"æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ (æœ€é«˜ç›¸ä¼¼åº¦:{max_similarity:.3f})", confidence
    else:
        if max_similarity < 0.55:
            return False, f"æœ€é«˜ç›¸ä¼¼åº¦ {max_similarity:.3f} è¿‡ä½", max_similarity
        elif avg_similarity < 0.40:
            return False, f"å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.3f} è¿‡ä½", max_similarity
        else:
            confidence = min(1.0, (max_similarity - 0.5) * 2.0)
            return True, f"æ–‡æ¡£ç›¸å…³æ€§è¶³å¤Ÿ (æœ€é«˜:{max_similarity:.3f}, å¹³å‡:{avg_similarity:.3f})", confidence

def hybrid_answering_strategy(question, relevant_docs, confidence):
    is_conceptual_question = any(keyword in question for keyword in 
                                ["ä»€ä¹ˆæ˜¯", "ä»€ä¹ˆå«", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š", "ä¸ºä»€ä¹ˆ"])
    
    if confidence > 0.7:
        strategy = "high_confidence_rag"
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ç›¸å…³ä¸Šä¸‹æ–‡ï¼š
{"\n\n".join([doc.page_content for doc in relevant_docs])}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡æä¾›å‡†ç¡®å›ç­”ï¼š"""
        
    elif confidence > 0.4:
        strategy = "balanced_hybrid" 
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥é€‚å½“ç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……ï¼š

ç›¸å…³ä¸Šä¸‹æ–‡ï¼š
{"\n\n".join([doc.page_content for doc in relevant_docs])}

é—®é¢˜ï¼š{question}

è¯·ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³å¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……ï¼š"""
        
    else:
        strategy = "model_primary"
        prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚æˆ‘çš„çŸ¥è¯†åº“ä¸­æœ‰ä¸€äº›å¯èƒ½ç›¸å…³çš„ä¿¡æ¯ï¼Œè¯·ä¸»è¦åŸºäºä½ çš„çŸ¥è¯†å›ç­”ï¼Œä½†å¯ä»¥å‚è€ƒè¿™äº›ä¿¡æ¯ï¼š

å¯èƒ½ç›¸å…³çš„ä¿¡æ¯ï¼š
{"\n\n".join([doc.page_content for doc in relevant_docs])}

é—®é¢˜ï¼š{question}

è¯·ä¸»è¦åŸºäºä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œå¦‚æœçŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æœ‰å¸®åŠ©å¯ä»¥å‚è€ƒï¼š"""
    
    return strategy, prompt

def init_vector_store(filepath=None, file_id=None, user_id=None, filename=None):
    global vector_store

    if not filepath:
        if not vector_store and os.path.exists('chroma_db'):
            vector_store = Chroma(
                persist_directory='chroma_db',
                embedding_function=embeddings
            )
            count = vector_store._collection.count()
            print(f"æˆåŠŸåŠ è½½æœ¬åœ°çŸ¥è¯†åº“ï¼Œå…± {count} æ¡æ–‡æ¡£å—")
        return

    try:
        print(f"æ­£åœ¨å¤„ç†: {filepath}, æ–‡ä»¶ID: {file_id}, ç”¨æˆ·ID: {user_id}, æ–‡ä»¶å: {filename}")

        if filepath.lower().endswith('.pdf'):
            loader = PyPDFLoader(filepath)
            documents = loader.load()
            print(f"PDF åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} é¡µ")
        else:
            with open(filepath, "rb") as f:
                raw = f.read()
                detected = chardet.detect(raw)
                encoding = detected['encoding'] or 'utf-8'
            encoding = 'utf-16' if 'utf-16' in encoding.lower() else encoding
            encoding = 'gbk' if 'gb' in encoding.lower() else encoding
            try:
                loader = TextLoader(filepath, encoding=encoding)
                documents = loader.load()
                print(f"æˆåŠŸåŠ è½½æ–‡æœ¬ï¼ˆ{encoding}ï¼‰: {len(documents)} æ®µ")
            except:
                loader = TextLoader(filepath, encoding="utf-8", errors="ignore")
                documents = loader.load()

        cleaned_docs = []
        for doc in documents:
            text = doc.page_content.replace('\ufeff', '').replace('\u200b', '').replace('\u3000', ' ').replace('\xa0', ' ').strip()
            if not text:
                text = f"ï¼ˆç©ºæ–‡æ¡£ï¼Œæ¥æºï¼š{os.path.basename(filepath)}ï¼‰"
            doc.page_content = text
            
            # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿æ–‡ä»¶IDè¢«æ­£ç¡®å­˜å‚¨
            # å¦‚æœfile_idä¸ºNoneï¼Œä»æ–‡ä»¶è·¯å¾„ä¸­æå–
            if file_id is None:
                file_id_from_path = os.path.basename(filepath).split('.')[0]
                doc.metadata['file_id'] = file_id_from_path
                print(f"ğŸ”„ ä»æ–‡ä»¶è·¯å¾„æå–file_id: {filepath} -> {file_id_from_path}")
            else:
                doc.metadata['file_id'] = file_id
            
            if user_id:
                doc.metadata['user_id'] = user_id
            if filename:
                doc.metadata['filename'] = filename
            
            # ç¡®ä¿sourceä¹Ÿè¢«æ­£ç¡®è®¾ç½®
            doc.metadata['source'] = filepath
                
            cleaned_docs.append(doc)

        text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=100)
        chunks = text_splitter.split_documents(cleaned_docs)
        if len(chunks) == 0:
            # åˆ›å»ºå ä½æ–‡æ¡£æ—¶ä¹Ÿè¦è®¾ç½®file_id
            placeholder_metadata = {"source": filepath}
            if file_id:
                placeholder_metadata['file_id'] = file_id
            chunks = [Document(page_content="ç©ºæ–‡æ¡£å ä½", metadata=placeholder_metadata)]

        print(f"æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(chunks)} å—")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªå—çš„metadataä½œä¸ºç¤ºä¾‹
        if chunks:
            print(f"ç¤ºä¾‹æ–‡æ¡£å—metadata: {chunks[0].metadata}")

        all_texts = [c.page_content for c in chunks]
        all_metadatas = [c.metadata for c in chunks]
        all_embeddings = []
        for i, text in enumerate(all_texts):
            embed_success = False
            for attempt in range(5):
                try:
                    embed = embeddings.embed_query(text)
                    all_embeddings.append(embed)
                    print(f"æ‰‹åŠ¨åµŒå…¥å— {i+1} æˆåŠŸ")
                    embed_success = True
                    break
                except Exception as e:
                    if "502" in str(e):
                        print(f"åµŒå…¥ 502ï¼Œé‡è¯•å— {i+1} ç¬¬ {attempt+1} æ¬¡...")
                        time.sleep(5)
                    else:
                        raise
            if not embed_success:
                raise Exception(f"åµŒå…¥å— {i+1} å¤±è´¥ï¼Œ5 æ¬¡é‡è¯•")

        if vector_store:
            vector_store.add_texts(
                texts=all_texts,
                embeddings=all_embeddings,
                metadatas=all_metadatas
            )
            print(f"æ–‡æ¡£å·²è¿½åŠ åˆ°çŸ¥è¯†åº“: {os.path.basename(filepath)}")
        else:
            class PrecomputedEmbeddings:
                def __init__(self, pre_embeds):
                    self.pre_embeds = pre_embeds

                def embed_documents(self, texts):
                    return self.pre_embeds

                def embed_query(self, text):
                    return self.pre_embeds[0]

            temp_embeddings = PrecomputedEmbeddings(all_embeddings)

            vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=temp_embeddings,
                persist_directory='chroma_db'
            )
            print(f"æ‰‹åŠ¨æ–°å»ºçŸ¥è¯†åº“æˆåŠŸï¼æ–‡æ¡£æ•°: {len(chunks)}")

        print(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {os.path.basename(filepath)}\n")

    except Exception as e:
        print(f"ä¸¥é‡é”™è¯¯ï¼æ–‡ä»¶å¤„ç†å½»åº•å¤±è´¥: {filepath}\né”™è¯¯ä¿¡æ¯: {str(e)}")
        raise

def enhanced_record_transaction(tx_type, from_user, to_user, amount, file_owner=None, file_id=None, question=None, details=None):
    """å¢å¼ºçš„äº¤æ˜“è®°å½•åŠŸèƒ½"""
    transactions = load_transactions()
    
    transaction = {
        'id': str(uuid.uuid4()),
        'type': tx_type,
        'from_user': from_user,
        'to_user': to_user,
        'amount': amount,
        'file_owner': file_owner,
        'file_id': file_id,
        'question': question,
        'details': details,  # æ–°å¢è¯¦ç»†ä¿¡æ¯å­—æ®µ
        'timestamp': datetime.now().isoformat()
    }
    
    transactions.append(transaction)
    save_transactions(transactions)
    
    # æ›´æ–°ç”¨æˆ·ä½™é¢
    users = load_users()
    if from_user in users and tx_type == 'spend':
        users[from_user]['coin_balance'] -= amount
        users[from_user]['total_spent'] += amount
    
    if to_user in users and tx_type == 'reward':
        users[to_user]['coin_balance'] += amount
        users[to_user]['total_earned'] += amount
    
    save_users(users)
    
    # è®°å½•è¯¦ç»†æ—¥å¿—
    log_transaction(transaction)

def log_transaction(transaction):
    """è®°å½•äº¤æ˜“æ—¥å¿—åˆ°æ–‡ä»¶"""
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        'transaction': transaction
    }
    
    log_file = 'transaction_logs.json'
    logs = []
    
    if os.path.exists(log_file):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        except:
            logs = []
    
    logs.append(log_entry)
    
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)
# ==================== Flask è·¯ç”± ====================

@app.route('/')
def index():
    if 'user_id' in session:
        return redirect('/dashboard')
    return render_template('index.html')

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        user_id = request.form.get('user_id', '').strip()
        password = request.form.get('password', '').strip()
        
        success, message = authenticate_user(user_id, password)
        if success:
            session['user_id'] = user_id
            return jsonify({'success': True, 'message': message})
        else:
            return jsonify({'success': False, 'message': message})
    
    return render_template('login.html')

@app.route('/register', methods=['GET', 'POST'])
def register():
    if request.method == 'POST':
        user_id = request.form.get('user_id', '').strip()
        password = request.form.get('password', '').strip()
        
        success, message = register_user(user_id, password)
        if success:
            session['user_id'] = user_id
            return jsonify({'success': True, 'message': message})
        else:
            return jsonify({'success': False, 'message': message})
    
    return render_template('register.html')

@app.route('/logout')
def logout():
    session.pop('user_id', None)
    return redirect('/')

@app.route('/dashboard')
def dashboard():
    if 'user_id' not in session:
        return redirect('/login')
    
    user_stats = get_user_stats(session['user_id'])
    shared_files = search_files(user_id=session['user_id'])
    
    vector_count = vector_store._collection.count() if vector_store else 0
    
    return render_template('dashboard.html', 
                         user_id=session['user_id'],
                         stats=user_stats,
                         files=shared_files,
                         vector_count=vector_count)

@app.route('/share', methods=['POST'])
def share_file():
    if 'user_id' not in session:
        return jsonify({'success': False, 'message': 'è¯·å…ˆç™»å½•'})
    
    filename = request.form.get('filename', '').strip()
    content = request.form.get('content', '').strip()
    authorize_rag = request.form.get('authorize_rag', 'false') == 'true'
    
    if not filename or not content:
        return jsonify({'success': False, 'message': 'æ–‡ä»¶åå’Œå†…å®¹ä¸èƒ½ä¸ºç©º'})
    
    file_id = save_shared_file(session['user_id'], filename, content, authorize_rag)
    
    return jsonify({
        'success': True, 
        'message': 'æ–‡ä»¶åˆ†äº«æˆåŠŸ',
        'file_id': file_id
    })



@app.route('/file_content/<file_id>')
def get_file_content(file_id):
    if 'user_id' not in session:
        return jsonify({'success': False, 'message': 'è¯·å…ˆç™»å½•'})
    
    files = load_files()
    if file_id not in files:
        return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'})
    
    file_info = files[file_id]
    
    return jsonify({
        'success': True,
        'filename': file_info['filename'],
        'content': file_info['content'],
        'upload_time': file_info['upload_time'],
        'user_id': file_info['user_id'],
        'authorize_rag': file_info.get('authorize_rag', False),
        'reference_count': file_info.get('reference_count', 0),
        'total_reward': file_info.get('total_reward', 0)
    })

@app.route('/ask')
def ask_stream():
    if 'user_id' not in session:
        return Response("data: è¯·å…ˆç™»å½•\n\n", mimetype='text/event-stream')
    
    user_id = session['user_id']
    question = request.args.get('q', '').strip()
    
    print(f"ç”¨æˆ· {user_id} æé—®: {question}")
    
    if not question:
        return Response("data: é—®é¢˜ä¸èƒ½ä¸ºç©º\n\n", mimetype='text/event-stream')
    
    users = load_users()
    if user_id not in users or users[user_id]['coin_balance'] < 0.000001:
        return Response("data: Coinä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼\n\n", mimetype='text/event-stream')
    
    def generate_response():
        should_use_rag = False
        rag_reason = ""
        confidence = 0.0
        relevant_docs = []
        
        try:
            conversation_cost = 0.000001
            record_transaction('spend', user_id, 'system', conversation_cost, None, None, question)
            
            current_balance = users[user_id]['coin_balance'] - conversation_cost
            print(f"ğŸ’° æœ¬æ¬¡å¯¹è¯æ¶ˆè€— {conversation_cost:.6f} coinï¼Œå½“å‰ä½™é¢: {current_balance:.6f} coin")
            
            if not vector_store or vector_store._collection.count() == 0:
                print("çŸ¥è¯†åº“ä¸ºç©ºï¼Œç›´æ¥åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”...")
                try:
                    response = llm.invoke(question)
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    # ç›´æ¥å‘é€å›ç­”å†…å®¹
                    yield f"data: {response_text}\n\n"
                    yield "data: [END]\n\n"
                except Exception as e:
                    yield f"data: LLM æœåŠ¡é”™è¯¯: {str(e)}\n\n"
                    yield "data: [END]\n\n"
                return

            print("çŸ¥è¯†åº“å·²åŠ è½½ï¼Œå¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            
            retriever = vector_store.as_retriever(search_kwargs={"k": 10})
            all_docs = retriever.invoke(question)
            
            print(f"ä»çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£å—")
            
            if not all_docs:
                print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”")
                try:
                    response = llm.invoke(question)
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    yield f"data: {response_text}\n\n"
                    yield "data: [END]\n\n"
                except Exception as e:
                    yield f"data: LLM æœåŠ¡é”™è¯¯: {str(e)}\n\n"
                    yield "data: [END]\n\n"
                return
            
            try:
                print("å¼€å§‹æ™ºèƒ½è¿‡æ»¤ç›¸å…³æ–‡æ¡£...")
                relevant_docs = adaptive_filter_relevant_docs(question, all_docs, embeddings, llm)
                print(f"è¿‡æ»¤åä¿ç•™ {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            except Exception as e:
                print(f"æ™ºèƒ½è¿‡æ»¤å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£")
                relevant_docs = all_docs
            
            try:
                should_use_rag, rag_reason, confidence = intelligent_rag_decision(question, relevant_docs)
                print(f"{rag_reason} (ç½®ä¿¡åº¦: {confidence:.2f})")
            except Exception as e:
                print(f"æ™ºèƒ½å†³ç­–å‡ºé”™: {str(e)}ï¼Œé»˜è®¤ä½¿ç”¨RAG")
                should_use_rag, rag_reason, confidence = True, "é»˜è®¤ä½¿ç”¨RAG", 0.5
            
            # å¥–åŠ±åˆ†é…ä¿¡æ¯åªåœ¨åç«¯æ˜¾ç¤º
            if relevant_docs and should_use_rag:
                try:
                    print(f"å¼€å§‹å¥–åŠ±åˆ†é…: ç”¨æˆ· {user_id}, é—®é¢˜ '{question}', ç›¸å…³æ–‡æ¡£ {len(relevant_docs)} ä¸ª")
                    reward_distribution = distribute_rewards(user_id, question, relevant_docs, conversation_cost)
                    
                    if reward_distribution:
                        print("å¥–åŠ±åˆ†é…è¯¦æƒ…ï¼š")
                        total_distributed = 0
                        
                        for file_id, reward_info in reward_distribution.items():
                            files = load_files()
                            file_info = files.get(file_id, {})
                            filename = file_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                            file_owner = file_info.get('user_id', 'æœªçŸ¥ç”¨æˆ·')
                            
                            reward_amount = reward_info['reward']
                            weight = reward_info['weight']
                            similarity = reward_info['similarity']
                            
                            total_distributed += reward_amount
                            
                            print(f"ğŸ“„ {filename} (ç”¨æˆ·: {file_owner})")
                            print(f"    ç›¸ä¼¼åº¦: {similarity:.3f} | æƒé‡: {weight:.3f} | å¥–åŠ±: {reward_amount:.8f} coin")
                        
                        print(f"ğŸ’° æ€»åˆ†é…é‡‘é¢: {total_distributed:.8f} coin")
                    else:
                        print("âš ï¸ æ²¡æœ‰è¿›è¡Œå¥–åŠ±åˆ†é…")
                        
                except Exception as e:
                    print(f"âŒ å¥–åŠ±åˆ†é…å‡ºé”™: {e}")
            
            # ğŸ¯ ä¿®å¤ï¼šä¼˜åŒ–AIå›ç­”ç”Ÿæˆéƒ¨åˆ†
            if should_use_rag and relevant_docs:
                try:
                    strategy, hybrid_prompt = hybrid_answering_strategy(question, relevant_docs, confidence)
                    print(f"ä½¿ç”¨å›ç­”ç­–ç•¥: {strategy}")

                    unique_sources = {}
                    for doc in relevant_docs:
                        src = doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
                        filename = os.path.basename(src)
                        # ğŸ¯ ä¿®æ”¹ï¼šå»æ‰æ–‡ä»¶æ‰©å±•åï¼Œåªæ˜¾ç¤ºæ–‡ä»¶å
                        filename_without_ext = os.path.splitext(filename)[0]
                        page = doc.metadata.get("page")
                        similarity = doc.metadata.get('semantic_similarity', 0)
                        
                        if filename not in unique_sources:
                            display_name = f"ã€Š{filename_without_ext}ã€‹"
                            if page is not None:
                                display_name += f" (ç¬¬ {page + 1} é¡µ)"
                            display_name += f" [ç›¸å…³åº¦:{similarity:.2f}]"
                            
                            unique_sources[filename] = {
                                'display': display_name,
                                'similarity': similarity
                            }
                    
                    # å‘é€ç›¸å…³æ–‡æ¡£ä¿¡æ¯åˆ°å‰ç«¯
                    if unique_sources:
                        yield "data: ğŸ“š æœ¬æ¬¡å›ç­”å‚è€ƒäº†ä»¥ä¸‹æ–‡æ¡£ï¼š\n\n"
                        sorted_sources = sorted(unique_sources.values(), key=lambda x: x['similarity'], reverse=True)
                        for i, info in enumerate(sorted_sources):
                            yield f"data: {i+1}. {info['display']}\n"
                        yield "data: \n\n"
                    
                    print("æ­£åœ¨ç”Ÿæˆå›ç­”...")
                    
                    # ğŸ¯ ä¿®å¤ï¼šæ·»åŠ è¶…æ—¶ä¿æŠ¤å’Œé”™è¯¯å¤„ç†
                    try:
                        # è®¾ç½®ç”Ÿæˆå›ç­”çš„è¶…æ—¶æ—¶é—´
                        import threading
                        from queue import Queue, Empty
                        
                        response_queue = Queue()
                        error_queue = Queue()
                        
                        def generate_ai_response():
                            try:
                                response = llm.invoke(hybrid_prompt)
                                response_text = response.content if hasattr(response, 'content') else str(response)
                                response_queue.put(response_text)
                            except Exception as e:
                                error_queue.put(str(e))
                        
                        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­ç”Ÿæˆå›ç­”
                        thread = threading.Thread(target=generate_ai_response)
                        thread.daemon = True
                        thread.start()
                        
                        # ç­‰å¾…å›ç­”ç”Ÿæˆï¼Œæœ€å¤šç­‰å¾…60ç§’
                        thread.join(timeout=60)
                        
                        if thread.is_alive():
                            # å¦‚æœè¶…æ—¶ï¼Œå‘é€è¶…æ—¶ä¿¡æ¯
                            yield "data: â° ç”Ÿæˆå›ç­”è¶…æ—¶ï¼Œè¯·é‡è¯•\n\n"
                        elif not error_queue.empty():
                            # å¦‚æœæœ‰é”™è¯¯ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                            error_msg = error_queue.get()
                            yield f"data: ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {error_msg}\n\n"
                        else:
                            # æˆåŠŸç”Ÿæˆå›ç­”
                            response_text = response_queue.get()
                            yield f"data: {response_text}\n\n"
                            
                    except Exception as e:
                        print(f"AIå›ç­”ç”Ÿæˆå¼‚å¸¸: {e}")
                        yield f"data: ç”Ÿæˆå›ç­”æ—¶å‡ºç°å¼‚å¸¸: {str(e)}\n\n"
                        # å°è¯•ç®€åŒ–å›ç­”
                        try:
                            simple_response = llm.invoke(f"è¯·ç®€å•å›ç­”ï¼š{question}")
                            simple_text = simple_response.content if hasattr(simple_response, 'content') else str(simple_response)
                            yield f"data: ç®€åŒ–å›ç­”: {simple_text}\n\n"
                        except:
                            yield "data: æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·é‡è¯•\n\n"
                    
                except Exception as e:
                    print(f"å›ç­”ç­–ç•¥å‡ºé”™: {e}")
                    yield f"data: å›ç­”ç­–ç•¥å‡ºé”™: {str(e)}\n\n"

# ==================== åœ¨ app.py çš„ ask_stream å‡½æ•°ä¸­æ‰¾åˆ°æ¨¡å‹è‡ªèº«çŸ¥è¯†å›ç­”éƒ¨åˆ† ====================

# æ›¿æ¢è¿™ä¸ª else åˆ†æ”¯ï¼ˆæ¨¡å‹è‡ªèº«çŸ¥è¯†å›ç­”éƒ¨åˆ†ï¼‰
            # ==================== æ›¿ä»£æ–¹æ¡ˆï¼šåˆå¹¶å›ç­”å’Œæç¤ºä¿¡æ¯ ====================

            else:
                print("å°†åŸºäºæ¨¡å‹è‡ªèº«çŸ¥è¯†è¿›è¡Œå›ç­”...")
                try:
                    enhanced_prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
                    
                    response = llm.invoke(enhanced_prompt)
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    
                    # ğŸ¯ ä¿®å¤ï¼šç›´æ¥åœ¨å›ç­”å†…å®¹ä¸­æ·»åŠ æç¤ºä¿¡æ¯
                    full_response = response_text + "\n\n---\n\nğŸ’¡ **æœ¬æ¬¡å›ç­”åŸºäºæ¨¡å‹çš„è®­ç»ƒçŸ¥è¯†**"
                    
                    # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                    import time
                    words = full_response.split(' ')
                    current_chunk = ""
                    
                    for i, word in enumerate(words):
                        current_chunk += word + " "
                        # æ¯4ä¸ªå•è¯æˆ–åˆ°è¾¾æœ«å°¾æ—¶å‘é€ä¸€æ¬¡
                        if i % 4 == 0 or i == len(words) - 1:
                            yield f"data: {current_chunk}\n\n"
                            current_chunk = ""
                            time.sleep(0.03)  # è½»å¾®å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæµå¼æ•ˆæœ
                    
                    yield "data: [END]\n\n"
                    
                except Exception as e:
                    yield f"data: ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}\n\n"
                    yield "data: [END]\n\n"
            yield "data: [END]\n\n"

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"AIå¯¹è¯é”™è¯¯è¯¦æƒ…: {error_details}")
            yield f"data: ç³»ç»Ÿé”™è¯¯: {str(e)}\n\n"
            yield "data: [END]\n\n"

    return Response(generate_response(), mimetype='text/event-stream')


@app.route('/community')
def community():
    if 'user_id' not in session:
        return redirect('/login')
    
    files = search_files()
    return render_template('community.html', files=files, session=session)

@app.route('/file_detail/<file_id>')
def file_detail(file_id):
    if 'user_id' not in session:
        return redirect('/login')
    
    files = load_files()
    if file_id not in files:
        return "æ–‡ä»¶ä¸å­˜åœ¨", 404
    
    file_info = files[file_id]
    
    return render_template('file_detail.html', 
                         file_info=file_info,
                         user_id=session['user_id'])

@app.route('/vector_status')
def vector_status():
    if 'user_id' not in session:
        return jsonify({'success': False, 'message': 'è¯·å…ˆç™»å½•'})
    
    if not vector_store:
        return jsonify({
            'success': True,
            'vector_count': 0,
            'status': 'æœªåˆå§‹åŒ–'
        })
    
    count = vector_store._collection.count()
    return jsonify({
        'success': True,
        'vector_count': count,
        'status': f'å·²åŠ è½½ {count} ä¸ªæ–‡æ¡£å—'
    })

@app.route('/reload_vector_store')
def reload_vector_store():
    if 'user_id' not in session:
        return jsonify({'success': False, 'message': 'è¯·å…ˆç™»å½•'})
    
    try:
        global vector_store  # è¿™æ˜¯æ­£ç¡®çš„ä½ç½®
        
        files = load_files()
        authorized_files = [file_info for file_info in files.values() if file_info.get('authorize_rag', False)]
        
        print(f"æ‰¾åˆ° {len(authorized_files)} ä¸ªæˆæƒæ–‡ä»¶éœ€è¦é‡æ–°åŠ è½½")
        
        if vector_store:
            import shutil
            if os.path.exists('chroma_db'):
                shutil.rmtree('chroma_db')
            vector_store = None
        
        for file_info in authorized_files:
            file_path = file_info.get('file_path')
            file_id = None
            for fid, finfo in files.items():
                if finfo == file_info:
                    file_id = fid
                    break
            user_id = file_info.get('user_id')
            filename = file_info.get('filename')
            
            if file_path and os.path.exists(file_path) and file_id:
                try:
                    add_file_to_vector_store(file_path, file_id, user_id, filename)
                    print(f"é‡æ–°åŠ è½½æ–‡ä»¶åˆ°çŸ¥è¯†åº“: {filename}")
                except Exception as e:
                    print(f"é‡æ–°åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        final_count = vector_store._collection.count() if vector_store else 0
        
        return jsonify({
            'success': True,
            'message': f'çŸ¥è¯†åº“é‡æ–°åŠ è½½å®Œæˆï¼Œå…± {final_count} ä¸ªæ–‡æ¡£å—',
            'vector_count': final_count
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'é‡æ–°åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}'
        })
    
@app.route('/health')
def health_check():
    status = {
        "ollama_status": "unknown",
        "embedding_model": "unknown", 
        "llm_model": "unknown",
        "vector_store": "empty" if not vector_store else f"loaded ({vector_store._collection.count()} docs)",
        "user_count": len(load_users()),
        "file_count": len(load_files())
    }
    
    try:
        test_embed = embeddings.embed_query("test")
        status["embedding_model"] = "ok"
        
        test_response = llm.invoke("hello")
        status["llm_model"] = "ok"
        status["ollama_status"] = "running"
        
    except Exception as e:
        status["ollama_status"] = f"error: {str(e)}"
    
    return jsonify(status)

@app.route('/files')
def list_files():
    if 'user_id' not in session:
        return jsonify({'success': False, 'message': 'è¯·å…ˆç™»å½•'})
    
    keyword = request.args.get('keyword', '').strip()
    file_id = request.args.get('file_id', '').strip()
    
    # ğŸ¯ ä¼˜åŒ–æœç´¢é€»è¾‘
    files = search_files(file_id=file_id if file_id else None, keyword=keyword)
    
    print(f"ğŸ” æœç´¢è¯·æ±‚ - å…³é”®è¯: '{keyword}', æ–‡ä»¶ID: '{file_id}', ç»“æœæ•°é‡: {len(files)}")
    
    return jsonify({
        'success': True,
        'files': files,
        'count': len(files)
    })

def search_files(file_id=None, user_id=None, keyword=None):
    """ä¼˜åŒ–æ–‡ä»¶æœç´¢åŠŸèƒ½"""
    files = load_files()
    results = []
    
    print(f"ğŸ” æœç´¢æ–‡ä»¶ - file_id: {file_id}, user_id: {user_id}, keyword: {keyword}")
    
    for fid, file_info in files.items():
        match = True
        
        if file_id and fid != file_id:
            match = False
        if user_id and file_info['user_id'] != user_id:
            match = False
        if keyword:
            keyword_lower = keyword.lower()
            # ğŸ¯ ä¼˜åŒ–ï¼šåœ¨æ–‡ä»¶åå’Œå†…å®¹ä¸­æœç´¢ï¼Œæé«˜æœç´¢å‡†ç¡®æ€§
            filename_match = keyword_lower in file_info['filename'].lower()
            content_match = keyword_lower in file_info['content'].lower()
            file_id_match = keyword_lower in fid.lower()
            user_id_match = keyword_lower in file_info['user_id'].lower()
            
            if not (filename_match or content_match or file_id_match or user_id_match):
                match = False
                
        if match:
            results.append({
                'file_id': fid,
                **file_info
            })
    
    # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—
    sorted_results = sorted(results, key=lambda x: x['upload_time'], reverse=True)
    
    print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(sorted_results)} ä¸ªæ–‡ä»¶")
    return sorted_results

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨å¤šç”¨æˆ·AIçŸ¥è¯†åº“å¹³å°...")
    print("ğŸ“š åˆå§‹åŒ–å‘é‡åº“...")
    init_vector_store()
    
    if vector_store:
        try:
            count = vector_store._collection.count()
            print(f"âœ… å‘é‡åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ å‘é‡åº“è®¿é—®é”™è¯¯: {e}")
    else:
        print("âš ï¸  å‘é‡åº“æœªåŠ è½½ï¼ŒçŸ¥è¯†åº“ä¸ºç©º")
    
    print("ğŸŒ å¯åŠ¨WebæœåŠ¡å™¨...")
    app.run(host='127.0.0.1', port=5000, debug=True)